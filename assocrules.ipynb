{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "news_item = pd.read_csv(\"ifeng_key.csv\", encoding=\"utf8\")     # 从csv文件中读取数据\n",
    "keywords = list(map(lambda x: x.split('/'), news_item[\"keywords\"]))   # 从数据中提取关键词数据\n",
    "del news_item     # 删除无用的变量\n",
    "key_df = pd.DataFrame(keywords)\n",
    "del keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>脱贫</td>\n",
       "      <td>攻坚</td>\n",
       "      <td>精准</td>\n",
       "      <td>扶贫</td>\n",
       "      <td>考核</td>\n",
       "      <td>攻坚战</td>\n",
       "      <td>习近平</td>\n",
       "      <td>2017</td>\n",
       "      <td>会议</td>\n",
       "      <td>贫困</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中非</td>\n",
       "      <td>非洲</td>\n",
       "      <td>习近平</td>\n",
       "      <td>合作</td>\n",
       "      <td>中纳</td>\n",
       "      <td>纳方</td>\n",
       "      <td>发展</td>\n",
       "      <td>中国</td>\n",
       "      <td>根哥布</td>\n",
       "      <td>纳米比亚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>金正恩</td>\n",
       "      <td>习近平</td>\n",
       "      <td>中朝</td>\n",
       "      <td>同志</td>\n",
       "      <td>两党</td>\n",
       "      <td>总书记</td>\n",
       "      <td>和平</td>\n",
       "      <td>委员长</td>\n",
       "      <td>发展</td>\n",
       "      <td>半岛</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2   3   4    5    6     7    8     9\n",
       "0   脱贫   攻坚   精准  扶贫  考核  攻坚战  习近平  2017   会议    贫困\n",
       "1   中非   非洲  习近平  合作  中纳   纳方   发展    中国  根哥布  纳米比亚\n",
       "2  金正恩  习近平   中朝  同志  两党  总书记   和平   委员长   发展    半岛"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_df.iloc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transaction_encoder(df, sparse = False):\n",
    "    key_column = np.sort(np.unique(df.unstack().dropna().values))   # 提取所有的关键词作为标签并去重\n",
    "    key_col_map = {}\n",
    "    for (index,), item in np.ndenumerate(key_column):   # 将关键词-位置数据对存入字典中\n",
    "        key_col_map[item] = index\n",
    "    # TODO(SHLLL): Use sparse array instead of normal one.\n",
    "    sparse = False\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    if(sparse):\n",
    "        for row_idx, row in df.iterrows():\n",
    "            for item in row.dropna().drop_duplicates():    # 去除None和重复的元素\n",
    "                col_idx = key_col_map[item]\n",
    "                indices.append(col_idx)\n",
    "            indptr.append(len(indices))\n",
    "        non_sparse_values = [1] * len(indices)\n",
    "        tr_array = csr_matrix((non_sparse_values, indices, indptr), dtype=bool)\n",
    "    #     df_tr = pd.SparseDataFrame(key_tr_array, columns=key_column)\n",
    "    else:\n",
    "        tr_array = np.zeros((df.shape[0], key_column.shape[0]), dtype=bool)\n",
    "        for row_idx, row in df.iterrows():\n",
    "            for item in row.dropna():     # 去除None\n",
    "                col_idx = key_col_map[item]\n",
    "                tr_array[row_idx, col_idx] = True\n",
    "    return key_column, tr_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_new_combinations(old_combinations):\n",
    "    # 将输入的项集扁平化并去重保存\n",
    "    item_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    # 取出原候选项集中的每一个项\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = max(old_combination)    # 取出该项中包含的最大列序号\n",
    "        for item in item_types_in_previous_step:  # 取出当前候选集每一个对应的列序号即关键词\n",
    "            if item > max_combination:\n",
    "                res = tuple(old_combination) + (item,)     # 组合形成新的候选项集\n",
    "                yield res\n",
    "                # TODO(SHLLL): 增加枝叶修剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_frequent_items(df, min_support=0.5, max_len=None):\n",
    "    # 这里进行了一项内容，即支持度的过滤\n",
    "    X = df.values     # 取Dataframe的值存取ndarray中\n",
    "    ary_col_idx = np.arange(X.shape[1])     # 创建一个列索引序列\n",
    "    support = (np.sum(X, axis=0) / float(X.shape[0]))      # 计算每个关键词出现的概率即支持度\n",
    "    support_dict = {1: support[support > min_support]}     # 将大于最小支持度的支持度存入字典\n",
    "    # 这里进行了支持度过滤即由候选集C1生成了频繁项集L1并存入字典中\n",
    "    itemset_dict = {1: ary_col_idx[support > min_support].reshape(-1, 1)}  # 取出支持度对应的编号\n",
    "    max_itemset = 1\n",
    "    rows_count = X.shape[0]\n",
    "    \n",
    "    if max_len == None:\n",
    "        max_len = float('inf')      # 设置max_len为无穷大\n",
    "    \n",
    "    while max_itemset and max_itemset < max_len:\n",
    "        next_max_itemset = max_itemset + 1\n",
    "        combin = _generate_new_combinations(itemset_dict[max_itemset])\n",
    "        frequent_items = []\n",
    "        frequent_items_support = []\n",
    "        \n",
    "        for c in combin:\n",
    "            together = X[:, c].all(axis=1)\n",
    "            support = together.sum() / rows_count    # 计算当前项的支持度\n",
    "            if support >= min_support:    # 提取当前候选项集中的频繁项\n",
    "                frequent_items.append(c)\n",
    "                frequent_items_support.append(support)\n",
    "        \n",
    "        if frequent_items:    # 如果找到了频繁项则将对应的项和支持度存入字典\n",
    "            itemset_dict[next_max_itemset] = np.array(frequent_items)\n",
    "            support_dict[next_max_itemset] = np.array(frequent_items_support)\n",
    "            max_itemset = next_max_itemset\n",
    "        else:     # 如果没有频繁项则表示当前已无可供寻找的频繁项\n",
    "            max_itemset = 0\n",
    "    return itemset_dict, support_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(df, min_support=0.5, use_colnames=False, find_rules=False, max_len=None):\n",
    "    itemset_dict, support_dict = _find_frequent_items(df, min_support, max_len)\n",
    "    if find_rules:\n",
    "        _find_asso_rules(df, itemset_dict, support_dict)\n",
    "    \n",
    "    all_fre = []\n",
    "    for k in sorted(itemset_dict):    # 取出字典中的key\n",
    "        support = pd.Series(support_dict[k])    # 取出对应的支持度\n",
    "        itemsets = pd.Series([i for i in itemset_dict[k]])   # 取出对应的关键词标号\n",
    "        res = pd.concat((support, itemsets), axis = 1)   # 横向拼接两个Series为Dataframe\n",
    "        all_fre.append(res)        # 将所有的Dataframe存入到List中\n",
    "    \n",
    "    fre_df = pd.concat(all_fre)    # 纵向拼接所有的Dataframe\n",
    "    fre_df.columns = [\"support\", \"itemsets\"]   # 为数据起一个标题\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for (idx,), item in np.ndenumerate(df.columns)}  # 创建一个索引--关键词名的mapping\n",
    "        fre_df[\"itemsets\"] = fre_df[\"itemsets\"].apply(lambda x: [mapping[i] for i in x])\n",
    "    fre_df = fre_df.reset_index(drop=True)\n",
    "    return fre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_asso_rules(df, itemset_dict, support_dict):\n",
    "    pass\n",
    "    \n",
    "#     for k in sorted(itemset_dict)[1:]:      # 从频繁二项集开始遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckey_column, key_tr_array = transaction_encoder(key_df)\n",
    "df_tr = pd.DataFrame(key_tr_array, columns=key_column)\n",
    "# df_tr.loc[:, (\"中非\", \"非洲\", \"习近平\")].head()\n",
    "res_df = apriori(df_tr, min_support=0.03, find_rules=True, max_len=4)       # 提取出现一次以上的关键词\n",
    "# res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = pd.Index([[1], [2], [3], [4], [5]])\n",
    "ser = pd.Series([1, 2, 3, 4, 5], index=ind)\n",
    "lis = []\n",
    "lis.append(ser)\n",
    "ind2 = pd.Index([[1,1], [2,2], [3,3], [4,4], [5,5]])\n",
    "ser2 = pd.Series([1, 2, 3, 4, 5], index=ind2)\n",
    "lis.append(ser2)\n",
    "ss = pd.concat(lis)\n",
    "print(ss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
